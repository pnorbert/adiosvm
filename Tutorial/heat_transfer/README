2D Heat Transfer example
Fortran 90 code, 2D array with 2D decomposition

1. Build
--------
Edit make.settings 
- compiler and compiler options
- ADIOS installation directory

$ make adios

2. Run
-------

$ ./heat_transfer 
 Usage: heat_transfer  output  N  M   nx  ny   steps iterations
 output: name of output file
 N:      number of processes in X dimension
 M:      number of processes in Y dimension
 nx:     local array size in X dimension per processor
 ny:     local array size in Y dimension per processor
 steps:  the total number of steps to output
 iterations: one step consist of this many iterations

$ mpirun -np 12 ./heat_transfer_adios1 heat  4 3    40 50   6 500

will perform the calculation 
- on 12 processes 
- on a 4*40 x 3*50 2D array (Fortran column-major ordering)
- for 6*500 iterations, 
- producing an output of 6 steps stored in heat.bp 

$ bpls -l heat.bp T
  double   T            6*{150, 160} = 0.000212009 / 999.47 / 442.536 / 318.995 
  double   dT           6*{150, 160} = 6.27403e-06 / 0.720097 / 0.135503 / 0.115625

$ ./plot_heat.sh heat.bp
$ eog . 


---------------
Staging demos
---------------
A 100 output step on less processes, with more computation between steps. 

If output method in heat_transfer.xml is FLEXPATH, 
you need two terminals. Run the staging reader-writer in one terminal and
the simulation in another terminal. The order of startup does not matter.

$ mpirun -np 2 stage_write/stage_write heat.bp staged.bp FLEXPATH "" MPI "" 2
$ mpirun -np 4 ./heat_transfer_adios2 heat 2 2 300 300 100 600

If output method in heat_transfer.xml is DATASPACES, 
you need three terminals. 
Run the DataSpaces server first in one terminal and tell the app 
the number of servers and clients.
Run the staging reader-writer in another terminal. 
Run the simulaton in yet another terminal.
Server must start first, the order of reader and writer does not matter.

$ /opt/dataspaces/bin/dataspaces_server -s 1 -c 6
$ mpirun -np 4 ./heat_transfer_adios2 heat 2 2 300 300 100 600
$ mpirun -np 2 stage_write/stage_write heat.bp staged.bp DATASPACES "" MPI "" 2



